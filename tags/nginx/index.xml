<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>nginx on 保罗札记</title>
    <link>https://www.zengxi.net/tags/nginx/</link>
    <description>Recent content in nginx on 保罗札记</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2008–2018, Steve Francia and the Hugo Authors; all rights reserved.</copyright>
    <lastBuildDate>Sun, 22 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://www.zengxi.net/tags/nginx/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Nginx服务器安全加固</title>
      <link>https://www.zengxi.net/posts/2020/11/nginx_security/</link>
      <pubDate>Sun, 22 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.zengxi.net/posts/2020/11/nginx_security/</guid>
      <description>基础知识 常用配置 顶部配置
1#定义 Nginx 运行的用户和用户组 2user nginx; 3 4#进程文件 5pid /var/run/nginx.pid; 6 7#错误日志位置和级别，debug、info、notice、warn、error、crit 8error_log /var/log/nginx/error.log warn; 9 10#Nginx worker 的进程数，一般可设置为可用的CPU内核数。 11worker_processes 8; 12 13#每个 worker 打开文件描述符的最大数量限制。理论值应该是最多打开文件数（系统的值ulimit -n） 14#与 nginx 进程数相除，但是 nginx 分配请求并不均匀，所以建议与ulimit -n的值保持一致。 15worker_rlimit_nofile 65535; Events 模块
1events { 2 #设置一个worker进程同时打开的最大连接数 3 worker_connections 2048; 4 5 #告诉nginx收到一个新连接通知后接受尽可能多的连接 6 multi_accept on; 7 8 #设置用于复用客户端线程的轮询方法。如果你使用Linux 2.6+，你应该使用epoll。 9 # 如果你使用*BSD，你应该使用kqueue。 10 use epoll; 11} HTTP 模块
1http { 2 #隐藏 Nginx 的版本号，提高安全性。 3 server_tokens off; 4 5 #开启高效文件传输模式，sendfile 指令指定 Nginx 是否调用sendfile 函数来输出文件， 6 #对于普通应用设为 on，如果用来进行下载等应用磁盘 IO 重负载应用，可设置为 off，以平 7 #衡磁盘与网络 I/O 处理速度，降低系统的负载。 8 sendfile on; 9 10 #是否开启目录列表访问，默认关闭。 11 autoindex off; 12 13 #告诉 Nginx 在一个数据包里发送所有头文件，而不一个接一个的发送 14 tcp_nopush on; 15 16 #告诉 Nginx 不要缓存数据，而是一段一段的发送--当需要及时发送数据时，就应该给应用设置 17 #这个属性，这样发送一小块数据信息时就不能立即得到返回值。Nginx 默认会始终工作在 tcp  18 #nopush 状态下。但是当开启前面的 sendfile on; 时，它的工作特点是 nopush 的最后一 19 #个包会自动转转换到 nopush off。为了减小那200ms的延迟，开启 nodelay on; 将其很快 20 #传送出去。结论就是 sendfile on; 开启时，tcp_nopush 和 tcp_nodelay 都是on 是可以的。 21 tcp_nodelay on; 22 23 #日志格式设定 24 log_format main &amp;#39;$remote_addr - $remote_user [$time_local] &amp;#34;$request&amp;#34; &amp;#39; 25 &amp;#39;$status $body_bytes_sent &amp;#34;$http_referer&amp;#34; &amp;#39; 26 &amp;#39;&amp;#34;$http_user_agent&amp;#34; &amp;#34;$http_x_forwarded_for&amp;#34;&amp;#39;; 27 #定义访问日志，设置为 off 可以关闭日志，提高性能 28 access_log /var/log/nginx/access.</description>
    </item>
    
    <item>
      <title>利用frp与nginx实现公网访问NAS</title>
      <link>https://www.zengxi.net/posts/2020/11/nas_frp_nginx/</link>
      <pubDate>Sat, 14 Nov 2020 11:45:39 +0000</pubDate>
      
      <guid>https://www.zengxi.net/posts/2020/11/nas_frp_nginx/</guid>
      <description>能够在远程访问存储NAS上面的资料，才能真正发挥NAS的作用。但是对于没有公网IP的内网用户，如何实现在公网上面访问NAS，是一个需要事先解决的问题。
如果拥有一个有公网IP的VPS，可以通过 frp + Nginx 来实现内网穿透解决这个问题。另外，从安全角度考虑，如果有可能的话，整个链路上面的都使用 https 协议通讯更加安全。
下面大致描绘了访问的链路图：
 浏览器通过https协议与 VPS 通讯。浏览器的请求先发到nginx上，ngnix再将请求转发的至frps。在frps前面加上nginx做反向代理的好处是，如果这个VPS上面有其他请求转发的需求（比如这个VPS部署了个人网站或者博客），那在访问所有这些服务的时候，都可以使用同一个端口（比如443）。使用默认端口的话，在访问的时候，端口号都可以不用输入 frps与部署在NAS内网的frpc通讯，frpc将请求转发至内网的NAS。  1(( browser )) ---https---&amp;gt; (( nginx --&amp;gt; frps )) ---https---&amp;gt;&amp;gt; (( frpc --&amp;gt; NAS )) VPS上的配置 配置frps 从 https://github.com/fatedier/frp/releases 上找到最新的版本并下载。解压后，修改frps.ini:
1[common] 2bind_port = 7000 3vhost_https_port = 7443 4# 使用kcp加速 5kcp_bind_port = 7001 6 7# auth 8authentication_method = token 9token = 12345678 10 11# log 12log_file = /var/log/frps.log 13log_level = info 14log_max_days = 3 15 下面注册frps为系统服务。如果是ubuntu系统，按如下格式创建一个新的文件 /etc/systemd/system/frps.</description>
    </item>
    
    <item>
      <title>配置Nginx支持长轮询</title>
      <link>https://www.zengxi.net/posts/2018/11/nginx-setting-for-long-polling/</link>
      <pubDate>Wed, 21 Nov 2018 17:13:06 +0000</pubDate>
      
      <guid>https://www.zengxi.net/posts/2018/11/nginx-setting-for-long-polling/</guid>
      <description>对于需要用到长轮询的web项目，可以在Nginx做一些配置来支持请求转发。分别在http与server块中加入下面的配置：
1http { 2 map $http_upgrade $connection_upgrade { 3 default upgrade; 4 &amp;#39;&amp;#39; close; 5 } 6} 7 8server { 9 location /your-api-url/ { 10 proxy_pass http://127.0.0.1:9000/your-api-url/; 11 proxy_http_version 1.1; 12 proxy_set_header Upgrade $http_upgrade; 13 proxy_set_header Connection $connection_upgrade; 14 proxy_set_header Host $http_host; 15 proxy_set_header X-Forwarded-Host $host; 16 proxy_set_header X-Forwarded-Server $host; 17 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 18 proxy_buffering off; 19 proxy_ignore_client_abort off; 20 } 21} 22 </description>
    </item>
    
    <item>
      <title>Docker Compose创建minio集群</title>
      <link>https://www.zengxi.net/posts/2018/07/docker-mino-cluster/</link>
      <pubDate>Wed, 18 Jul 2018 23:00:47 +0000</pubDate>
      
      <guid>https://www.zengxi.net/posts/2018/07/docker-mino-cluster/</guid>
      <description>Minio是一个简单易用的轻量级对象存储服务，同时它也支持集群环境。使用Minio的docker镜像可以快速地搭建集群环境。
下面是docker-compose.yml文件的示例。分布式的Minio服务至少需要4个节点，所以在docker-compose.yml文件中，至少要配置4个服务。每个服务的command配置必须一样，保证集群环境正常运行。
1version:&amp;#39;3&amp;#39;2services:3minio-node1:4image:minio/minio 5hostname:minio-node1 6ports:7- &amp;#34;29001:9000&amp;#34;8volumes:9- ./node1/data:/data10env_file:./key.env11command:server http://minio-node1/data http://minio-node2/data http://minio-node3/data http://minio-node4/data 12minio-node2:13image:minio/minio14hostname:minio-node215ports:16- &amp;#34;29002:9000&amp;#34;17volumes:18- ./node2/data:/data19env_file:./key.env20command:server http://minio-node1/data http://minio-node2/data http://minio-node3/data http://minio-node4/data21minio-node3:22image:minio/minio23hostname:minio-node324ports:25- &amp;#34;29003:9000&amp;#34;26volumes:27- ./node3/data:/data28env_file:./key.env29command:server http://minio-node1/data http://minio-node2/data http://minio-node3/data http://minio-node4/data30minio-node4:31image:minio/minio32hostname:minio-node433ports:34- &amp;#34;29004:9000&amp;#34;35volumes:36- ./node4/data:/data37env_file:./key.env38command:server http://minio-node1/data http://minio-node2/data http://minio-node3/data http://minio-node4/data上面的配置中env_file指定的加载环境变量文件key.env，这里是为了设置登录minio的用户名密码。下面的key.env示例：
1MINIO_ACCESS_KEY=testkey 2MINIO_SECRET_KEY=testpassword 通过上述的docker-compose.yml文件，就可以启动一个minio的集群环境。可以做一些测试，通过浏览器访问 http://localhost:29001 ，新建bucket并上传一个文件，然后在 http://localhost:29002 上面也可以正常访问。同时，可以特地停掉或者启用其中一些服务，用来测试minio集群环境的高可用性。
如果其中某一个节点服务不可用，这就意味着无法向该节点发起请求来操作对象。比如上述的节点1挂掉， 就无法通过浏览器来访问 http://localhost:29001。 从高可用和负载均衡角度来讲，必须通过负载均衡器来转发请求到各个节点，而不是直接访问某个节点的服务。可以在nginx上做个简单的配置来实现。
下面是nginx配置的示例。配置完重启nginx服务后，就可以使用 http://localhost:29000 来访问和操作对象
1upstream minio_servers { 2 server 127.0.0.1:29001; 3 server 127.0.0.1:29002; 4 server 127.0.0.1:29003; 5 server 127.0.0.1:29004; 6} 7 8server { 9 listen 29000; 10 server_name localhost; 11 12 location / { 13 proxy_set_header Host $http_host; 14 proxy_pass http://minio_servers; 15 } 16} 17 </description>
    </item>
    
  </channel>
</rss>
